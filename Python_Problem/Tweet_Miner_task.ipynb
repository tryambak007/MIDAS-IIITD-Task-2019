{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_Miner_task.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gcO6iYsxgS-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the onset, we need to import required libraries. Tweepy is a library that has been used here to fetch tweets from the website. Pandas is useful for printing the final result in tabular form. Json library is used for handling json data."
      ]
    },
    {
      "metadata": {
        "id": "akzGaXjWDIxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import json\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kv2adBsnxex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we store the different keys and tokens which is needed for accessing tweets from the website. All the keys have been replaced by * for security reasons."
      ]
    },
    {
      "metadata": {
        "id": "5-6_Qr1cEU-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "consumer_key = \"**\"\n",
        "consumer_secret = \"**\"\n",
        "access_token = \"**\"\n",
        "access_secret = \"**\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMmq5BpRoFRu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following block sets all required keys. Then, a call to the API is made which will handle the tweets."
      ]
    },
    {
      "metadata": {
        "id": "y_XEmQWUG1pp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "twitter_api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nu__zqA5oeBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This next segment fetches all tweet data.  The variable tweets is used to iterate through the status objects of the cursor. Each status object's json data is stored in \"._json\" attribute. The json data in each status object is dumped as string in the file \"twitter_data.jsonl\". Additionaly a newline is also appended after each json data dump.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "31IvU4aJQD9-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('twitter_data.jsonl', 'w') as dumpdata:\n",
        "  for tweets in tweepy.Cursor(twitter_api.user_timeline, screen_name='@midasIIITD', tweet_mode=\"extended\").items():\n",
        "      json.dump(tweets._json, dumpdata)\n",
        "      dumpdata.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGJeHDuZWIT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Moving on to the next part of the code, each line of the previously created file is read and stored in a list of strings."
      ]
    },
    {
      "metadata": {
        "id": "7kJwgr_QRnAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open('twitter_data.jsonl', 'r') as infile:\n",
        "    content = infile.readlines()                    # content is a list of string storing each line of the file\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgHvFVSwZJnz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A function is created which fetches the number of images in a tweet. Unlike other data, image may be stored in two location depending on the type of the tweet. If the tweet is actually a retweet, the image data is stored inside a list named \"*media*\" which is present inside a dictionary whose key is named \"*entities*\" which again is present inside a dictionary whose key is named \"*retweeted_status*\". In case the tweet is not a retweet the image is stored in list *\"media\"* which is stored inside a dictionary whose key is *\"entities\"*.\n",
        "\n",
        "The function *get_imagecount()* takes the list *media* as a parameter and checks whether any of the list entry (which again are dictionaries) has \"type\" key value \"photo\". If yes, it is a image, if no, then it is not.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XFTVx8LPPaRx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_imagecount(medialist=[]):\n",
        "  count = 0\n",
        "  for diction in medialist:\n",
        "    if diction[\"type\"] == \"photo\":\n",
        "      count = count+1\n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mmFVyGyGhA0o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The required data is fetched from the json dictionaries and stored in corresponding lists as strings.\n",
        "Here too, if the tweet is a retweet, favorite count and retweet count is present in a dictionary with key \"retweeted_status\". If it is just a simple tweet, they are present in the main dictionary itself"
      ]
    },
    {
      "metadata": {
        "id": "JnUFlY76RsPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textlist = []                             # list to store text of tweet\n",
        "fav_count = []                            # list to store number of likes on tweet\n",
        "retweet_count = []                        # list to store number of retweets\n",
        "datelist = []                             # list to store date and time of tweet\n",
        "image_count = []                          # list to store number of images in a tweet\n",
        "for line in content:\n",
        "  jsondata = json.loads(line)                       # converting from string to type dictionary\n",
        "  isRetweet = False                                 # to check whether a tweet is a retweet or not \n",
        "  textlist.append(jsondata[\"full_text\"])            \n",
        "  try:\n",
        "    fav_count.append(jsondata[\"retweeted_status\"][\"favorite_count\"])                  # if the tweet is a retweet, favorite and retweet count is stored in a \n",
        "    retweet_count.append(jsondata[\"retweeted_status\"][\"retweet_count\"])               # dictionary whose key is \"retweeted_status\". In case the tweet is a retweet,\n",
        "    isRetweet = True                                                                  # try block gets successfully executed\n",
        "  except:\n",
        "    fav_count.append(jsondata[\"favorite_count\"])                                      # in case the tweet is not a retweet, the try block causes an error and execution\n",
        "    retweet_count.append(jsondata[\"retweet_count\"])                                   # moves into except block\n",
        "    isRetweet = False\n",
        "  datelist.append(jsondata[\"created_at\"])\n",
        "  try :\n",
        "    if isRetweet == False:                                                            # As explained in a previous markdown cell, this try block attempts to get \n",
        "      if get_imagecount(jsondata[\"entities\"][\"media\"]) == 0 :                         # number of images based on whether a tweet is a retweet or not\n",
        "         image_count.append(\"None\")\n",
        "      else :\n",
        "         image_count.append(get_imagecount(jsondata[\"entities\"][\"media\"]))\n",
        "    else:\n",
        "      if get_imagecount(jsondata[\"retweeted_status\"][\"entities\"][\"media\"]) == 0 :     # if there is no media, \"None\" is appended\n",
        "         image_count.append(\"None\")\n",
        "      else :\n",
        "         image_count.append(get_imagecount(jsondata[\"retweeted_status\"][\"entities\"][\"media\"]))\n",
        "  except :\n",
        "    image_count.append(\"None\")                                                        # if the key \"media\" is absent, it suggests 0 images and \"None\" is appended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTYeycjnl9dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All the lists are put inside a dictionary with appropriate key names for conversion to a dataframe of pandas."
      ]
    },
    {
      "metadata": {
        "id": "Fu7kNWGzYGrQ",
        "colab_type": "code",
        "outputId": "fb29f1e1-48b9-43d3-b393-0b33397b563c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "table_data = {'Text': textlist, 'Likes' : fav_count, 'Retweets' : retweet_count, 'Images': image_count, 'Date/Time': datelist}\n",
        "print(len(textlist))\n",
        "print(len(fav_count))\n",
        "print(len(image_count))\n",
        "print(len(datelist))\n",
        "print(len(retweet_count))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "344\n",
            "344\n",
            "344\n",
            "344\n",
            "344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uDcaTPVnmrpa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dictionary is converted into a dataframe so that it can be displayed in a tabular format."
      ]
    },
    {
      "metadata": {
        "id": "9GugfvqP28dp",
        "colab_type": "code",
        "outputId": "5dd95916-8f10-4ede-d02d-3f80950cb83d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2176
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(table_data)\n",
        "print(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          Date/Time Images  Likes  Retweets  \\\n",
            "0    Wed Apr 10 09:01:29 +0000 2019   None      0         0   \n",
            "1    Wed Apr 10 04:51:26 +0000 2019      1      3         1   \n",
            "2    Tue Apr 09 16:45:07 +0000 2019      1     59        14   \n",
            "3    Tue Apr 09 05:04:27 +0000 2019   None     96        35   \n",
            "4    Tue Apr 09 05:04:11 +0000 2019      1     37        17   \n",
            "5    Mon Apr 08 19:38:09 +0000 2019   None     20        15   \n",
            "6    Mon Apr 08 07:08:12 +0000 2019      1     18         2   \n",
            "7    Mon Apr 08 03:27:42 +0000 2019      1      5         0   \n",
            "8    Sun Apr 07 14:17:29 +0000 2019   None      0         0   \n",
            "9    Sun Apr 07 14:17:09 +0000 2019   None      0         0   \n",
            "10   Sun Apr 07 11:43:24 +0000 2019   None      1         1   \n",
            "11   Sun Apr 07 06:55:19 +0000 2019   None      5         2   \n",
            "12   Sun Apr 07 06:53:38 +0000 2019   None      4         1   \n",
            "13   Sun Apr 07 05:32:27 +0000 2019   None      6         1   \n",
            "14   Sun Apr 07 05:29:40 +0000 2019   None      7         1   \n",
            "15   Sat Apr 06 17:11:29 +0000 2019   None     21         2   \n",
            "16   Sat Apr 06 16:43:27 +0000 2019      1     18         3   \n",
            "17   Fri Apr 05 16:08:37 +0000 2019   None     11         1   \n",
            "18   Fri Apr 05 04:05:11 +0000 2019   None     49        16   \n",
            "19   Fri Apr 05 04:04:43 +0000 2019      1     21        11   \n",
            "20   Wed Apr 03 18:31:53 +0000 2019      1    225        59   \n",
            "21   Wed Apr 03 17:04:32 +0000 2019      1   2422       872   \n",
            "22   Wed Apr 03 09:03:40 +0000 2019   None    151        16   \n",
            "23   Wed Apr 03 07:46:02 +0000 2019   None      4         4   \n",
            "24   Tue Apr 02 04:20:13 +0000 2019   None      8         1   \n",
            "25   Tue Apr 02 02:44:54 +0000 2019   None      5         1   \n",
            "26   Tue Apr 02 02:35:44 +0000 2019   None      9         7   \n",
            "27   Mon Apr 01 06:53:08 +0000 2019   None      7         2   \n",
            "28   Sun Mar 31 10:21:24 +0000 2019      1     12        10   \n",
            "29   Fri Mar 29 19:43:24 +0000 2019   None      7         2   \n",
            "..                              ...    ...    ...       ...   \n",
            "314  Thu Aug 09 05:59:57 +0000 2018   None    567       265   \n",
            "315  Wed Aug 08 11:30:56 +0000 2018   None      2         1   \n",
            "316  Wed Aug 08 05:53:48 +0000 2018      1      3         1   \n",
            "317  Wed Aug 08 05:45:58 +0000 2018   None      6         1   \n",
            "318  Tue Aug 07 07:16:33 +0000 2018      1     88        40   \n",
            "319  Tue Aug 07 02:05:12 +0000 2018   None      1         1   \n",
            "320  Tue Aug 07 01:58:49 +0000 2018   None      1         1   \n",
            "321  Tue Aug 07 01:50:33 +0000 2018   None      1         1   \n",
            "322  Mon Aug 06 17:48:23 +0000 2018   None      0         0   \n",
            "323  Mon Aug 06 17:46:59 +0000 2018      1    423       103   \n",
            "324  Mon Aug 06 06:06:47 +0000 2018   None      0         0   \n",
            "325  Fri Aug 03 05:56:33 +0000 2018   None      6         1   \n",
            "326  Wed Aug 01 11:47:15 +0000 2018      1      5         1   \n",
            "327  Wed Aug 01 11:20:07 +0000 2018   None      7         4   \n",
            "328  Wed Aug 01 05:06:47 +0000 2018      1      4         4   \n",
            "329  Tue Jul 31 12:11:52 +0000 2018   None      1         0   \n",
            "330  Tue Jul 31 02:06:26 +0000 2018   None    756       264   \n",
            "331  Mon Jul 30 07:30:51 +0000 2018   None     14         2   \n",
            "332  Sat Jul 28 11:07:11 +0000 2018   None    218        57   \n",
            "333  Sat Jul 28 06:14:09 +0000 2018      1    225       105   \n",
            "334  Sat Jul 28 06:13:48 +0000 2018   None      7         7   \n",
            "335  Sat Jul 28 04:08:21 +0000 2018      1     36         6   \n",
            "336  Fri Jul 27 06:46:44 +0000 2018   None      3         2   \n",
            "337  Fri Jul 27 04:07:31 +0000 2018   None      8         2   \n",
            "338  Wed Jul 25 05:14:35 +0000 2018   None      5         1   \n",
            "339  Tue Jul 24 10:33:23 +0000 2018   None      2         1   \n",
            "340  Tue Jul 24 10:12:34 +0000 2018   None      2         1   \n",
            "341  Tue Jul 24 09:46:26 +0000 2018      1      4         1   \n",
            "342  Mon Jul 23 16:25:05 +0000 2018   None      3         1   \n",
            "343  Mon Jul 23 12:53:15 +0000 2018   None      7         4   \n",
            "\n",
            "                                                  Text  \n",
            "0    Clarification: Our earlier post which indicate...  \n",
            "1    RT @IIITDelhi: Applications open for MTech (CB...  \n",
            "2    RT @IIITDelhi: We are delighted to share that ...  \n",
            "3    RT @Harvard: Professor Jelani Nelson founded A...  \n",
            "4    RT @emnlp2019: For anyone interested in submit...  \n",
            "5    RT @multimediaeval: Announcing the 2019 MediaE...  \n",
            "6    Many Congratulations to @midasIIITD student, S...  \n",
            "7    @midasIIITD thanks all students who have appea...  \n",
            "8    @himanchalchandr Meanwhile, complete CV/NLP ta...  \n",
            "9    @sayangdipto123 Submit as per the guideline ag...  \n",
            "10   We request all students whose interview are sc...  \n",
            "11   Other queries: \"none of the Tweeter Apis give ...  \n",
            "12   Other queries: \"do we have to make two differe...  \n",
            "13   Other queries: \"If using Twitter api, it does ...  \n",
            "14   Response to some queries asked by students on ...  \n",
            "15   RT @kdnuggets: Top 8 #Free Must-Read #Books on...  \n",
            "16   @nupur_baghel @PennDATS Congratulation @nupur_...  \n",
            "17   We have emailed the task details to all candid...  \n",
            "18   RT @rfpvjr: Our NAACL paper on polarization in...  \n",
            "19   RT @kdnuggets: Effective Transfer Learning For...  \n",
            "20   RT @stanfordnlp: What’s new in @Stanford CS224...  \n",
            "21   RT @DeepMindAI: Today we're releasing a large-...  \n",
            "22   RT @ylecun: Congratulations Jitendra Malik !\\n...  \n",
            "23   RT @IIITDelhi: Another chance to take admissio...  \n",
            "24   Dear @midasIIITD internship candidates who hav...  \n",
            "25   Looking forward to your paper submission to @I...  \n",
            "26   RT @ngrams: Reproducibility in multimedia rese...  \n",
            "27   Online application for https://t.co/DJFDrQsHZP...  \n",
            "28   RT @ACMMM19: A final reminder of the Reproduci...  \n",
            "29   RT @isarth23: Thanks for the support and help ...  \n",
            "..                                                 ...  \n",
            "314  RT @TensorFlow: TensorFlow 1.10.0 has been rel...  \n",
            "315  @midasIIITD is looking for motivated IIITD MTe...  \n",
            "316  @IIITDelhi @ponguru @RatnRajiv The results of ...  \n",
            "317  RT @IIITDelhi: @midasIIITD has secured rank 1 ...  \n",
            "318  RT @kdnuggets: Comparison of Top 6 Python NLP ...  \n",
            "319  Check more details of the 20th IEEE Internatio...  \n",
            "320  MR2AMC@ISM 2018 will be organized by @RatnRaji...  \n",
            "321  Our workshop proposal named, \"MR2AMC: Multimod...  \n",
            "322  @NUSComputing Congratulations Abdelhak and Pro...  \n",
            "323  RT @goodfellow_ian: One of the most anticipate...  \n",
            "324     @the_dhumketu Great to have you in @midasIIITD  \n",
            "325  Congratulation @soujanyaporia for being appoin...  \n",
            "326  @IIITDelhi @the_dhumketu Thanks team @midasIII...  \n",
            "327  RT @IIITDelhi: Congratulations @midasIIITD int...  \n",
            "328  RT @learning_pt: Profile of the 5 Indian under...  \n",
            "329  Have a look at the list of accepted papers in ...  \n",
            "330  RT @goodfellow_ian: https://t.co/hYiWI7ntyk Te...  \n",
            "331  RT @IIITDelhi: Congratulations Dr. @RatnRajiv ...  \n",
            "332  RT @ylecun: Jitendra Malik, who directs FAIR-M...  \n",
            "333  RT @kdnuggets: .@Bloomberg launches free cours...  \n",
            "334  RT @TechAtBloomberg: Missed #PyLondinium18? Wa...  \n",
            "335  RT @IIITDelhi: We are delighted to announce th...  \n",
            "336  Get ready for the annual technical fest of @II...  \n",
            "337  Congratulations Dr. @RatnRajiv and team @midas...  \n",
            "338  Congratulations MIDAS @midasIIITD intern Prakh...  \n",
            "339    MIDAS@IIITD foundation. https://t.co/LKuzyBHzjm  \n",
            "340  It feels great to be the part of @IIITDelhi. h...  \n",
            "341  Thank you, @toonzratn for designing the logo o...  \n",
            "342  We are on Facebook too. Like our page to get o...  \n",
            "343  MIDAS is a group of researchers at IIIT-Delhi ...  \n",
            "\n",
            "[344 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j6S65_qPiiRI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For easy viewing and modification, the dataframe can be saved into a **.csv** too."
      ]
    },
    {
      "metadata": {
        "id": "Ui8Ti7kmtaYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv(\"mined_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}